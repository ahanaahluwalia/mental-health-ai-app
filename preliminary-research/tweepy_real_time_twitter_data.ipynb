{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweepy_real_time_twitter_data",
      "provenance": [],
      "authorship_tag": "ABX9TyNBvlnQ59ayGsMOLCPpxrd1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joudelshawa/mental-health-ai-app/blob/feature%2Fdata_preprocessing/tweepy_real_time_twitter_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrWHUDB2xUrR"
      },
      "source": [
        "!pip install tweepy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA4FAeMLx23p"
      },
      "source": [
        "import tweepy\n",
        "from pymongo import MongoClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIhCAt1ByavX"
      },
      "source": [
        "# import modules\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "  \n",
        "  \n",
        "# function to display data of each tweet\n",
        "def printtweetdata(n, ith_tweet):\n",
        "    print()\n",
        "    print(f\"Tweet {n}:\")\n",
        "    print(f\"Username:{ith_tweet[0]}\")\n",
        "    print(f\"Description:{ith_tweet[1]}\")\n",
        "    print(f\"Location:{ith_tweet[2]}\")\n",
        "    print(f\"Following Count:{ith_tweet[3]}\")\n",
        "    print(f\"Follower Count:{ith_tweet[4]}\")\n",
        "    print(f\"Total Tweets:{ith_tweet[5]}\")\n",
        "    print(f\"Retweet Count:{ith_tweet[6]}\")\n",
        "    print(f\"Tweet Text:{ith_tweet[7]}\")\n",
        "    print(f\"Hashtags Used:{ith_tweet[8]}\")\n",
        "  \n",
        "  \n",
        "# function to perform data extraction\n",
        "def scrape(words, date_since, numtweet):\n",
        "      \n",
        "    # Creating DataFrame using pandas\n",
        "    db = pd.DataFrame(columns=['username', 'description', 'location', 'following',\n",
        "                               'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags'])\n",
        "      \n",
        "    # We are using .Cursor() to search through twitter for the required tweets.\n",
        "    # The number of tweets can be restricted using .items(number of tweets)\n",
        "    tweets = tweepy.Cursor(api.search, q=words, lang=\"en\",\n",
        "                           since=date_since, tweet_mode='extended').items(numtweet)\n",
        "     \n",
        "    # .Cursor() returns an iterable object. Each item in \n",
        "    # the iterator has various attributes that you can access to \n",
        "    # get information about each tweet\n",
        "    list_tweets = [tweet for tweet in tweets]\n",
        "      \n",
        "    # Counter to maintain Tweet Count\n",
        "    i = 1  \n",
        "      \n",
        "    # we will iterate over each tweet in the list for extracting information about each tweet\n",
        "    for tweet in list_tweets:\n",
        "        username = tweet.user.screen_name\n",
        "        description = tweet.user.description\n",
        "        location = tweet.user.location\n",
        "        following = tweet.user.friends_count\n",
        "        followers = tweet.user.followers_count\n",
        "        totaltweets = tweet.user.statuses_count\n",
        "        retweetcount = tweet.retweet_count\n",
        "        hashtags = tweet.entities['hashtags']\n",
        "          \n",
        "        # Retweets can be distinguished by a retweeted_status attribute,\n",
        "        # in case it is an invalid reference, except block will be executed\n",
        "        try:\n",
        "            text = tweet.retweeted_status.full_text\n",
        "        except AttributeError:\n",
        "            text = tweet.full_text\n",
        "        hashtext = list()\n",
        "        for j in range(0, len(hashtags)):\n",
        "            hashtext.append(hashtags[j]['text'])\n",
        "          \n",
        "        # Here we are appending all the extracted information in the DataFrame\n",
        "        ith_tweet = [username, description, location, following,\n",
        "                     followers, totaltweets, retweetcount, text, hashtext]\n",
        "        db.loc[len(db)] = ith_tweet\n",
        "          \n",
        "        # Function call to print tweet data on screen\n",
        "        printtweetdata(i, ith_tweet)\n",
        "        i = i+1\n",
        "    filename = 'scraped_tweets.csv'\n",
        "      \n",
        "    # we will save our database as a CSV file.\n",
        "    db.to_csv(filename)\n",
        "  \n",
        "  \n",
        "if __name__ == '__main__':\n",
        "      \n",
        "    # Enter your own credentials obtained \n",
        "    # from your developer account\n",
        "    consumer_key = \"mdmzMBrRM2nNnTdEoN7jZuxmY\"\n",
        "    consumer_secret = \"3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce\"\n",
        "    access_key = \"525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP\"\n",
        "    access_secret = \"eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV\"\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "      \n",
        "    # Enter Hashtag and initial date\n",
        "    print(\"Enter Twitter HashTag to search for\")\n",
        "    words = input()\n",
        "    print(\"Enter Date since The Tweets are required in yyyy-mm--dd\")\n",
        "    date_since = input()\n",
        "      \n",
        "    # number of tweets you want to extract in one run\n",
        "    numtweet = 100  \n",
        "    scrape(words, date_since, numtweet)\n",
        "    print('Scraping has completed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydIjycWTx6mP"
      },
      "source": [
        "consumer_key = \"mdmzMBrRM2nNnTdEoN7jZuxmY\"\n",
        "consumer_secret = \"3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce\"\n",
        "access_token = \"525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP\"\n",
        "access_token_secret = \"eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPGqSMSYx9YO"
      },
      "source": [
        "def get_all_tweets(screen_name):\n",
        "    client = MongoClient()\n",
        "    db = client.db_twitter\n",
        "    tweets = db.Tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jgkw9MLyA8E"
      },
      "source": [
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ2aeI0HyHFF"
      },
      "source": [
        "# initialize a list to hold all the tweepy Tweets\n",
        "alltweets = []\n",
        "\n",
        "# make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "new_tweets = api.user_timeline(screen_name=get_all_tweets, count=200)\n",
        "\n",
        "# save most recent tweets\n",
        "alltweets.extend(new_tweets)\n",
        "\n",
        "# save the id of the oldest tweet less one\n",
        "oldest = alltweets[-1].id - 1\n",
        "\n",
        "# keep grabbing tweets until there are no tweets left to grab\n",
        "while len(new_tweets) > 0:\n",
        "    print\n",
        "    \"getting tweets before %s\" % (oldest)\n",
        "\n",
        "    # all subsiquent requests use the max_id param to prevent duplicates\n",
        "    new_tweets = api.user_timeline(screen_name=get_all_tweets, count=200, max_id=oldest)\n",
        "\n",
        "    # save most recent tweets\n",
        "    alltweets.extend(new_tweets)\n",
        "\n",
        "    # update the id of the oldest tweet less one\n",
        "    oldest = alltweets[-1].id - 1\n",
        "\n",
        "   # print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
        "\n",
        "# transform the tweepy tweets into a 2D array that will populate the csv\n",
        "outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "550jnyP4yROY"
      },
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import tweepy  \n",
        "import csv\n",
        "import datetime\n",
        "consumer_key = \"mdmzMBrRM2nNnTdEoN7jZuxmY\"\n",
        "consumer_secret = \"3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce\"\n",
        "access_key = \"525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP\"\n",
        "access_secret = \"eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV\"\n",
        "api = tweepy.API(auth)\n",
        "#Add time your file was created to discriminate date/time after running the crontab file\n",
        "#filename = 'twitter_data_analysis'+(datetime.datetime.now().strftime(\"%Y-%m-%d-%H\"))+'.csv'\n",
        "#with open (filename, 'a+', newline='') as csvFile:\n",
        " #  csvWriter = csv.writer(csvFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewAwu0TH0e4d"
      },
      "source": [
        "#auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "#auth.set_access_token(access_key, access_secret)\n",
        "#api = tweepy.API(auth)\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eA-OPw60mHv"
      },
      "source": [
        "# Define the search term and the date_since date as variables\n",
        "search_words = \"#wildfires\"\n",
        "date_since = \"2018-11-16\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIQV8bXC1cYG"
      },
      "source": [
        "tweets = tw.Cursor(api.search,\n",
        "              q=search_words,\n",
        "              lang=\"en\",\n",
        "              since=date_since).items(5)\n",
        "tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiZXFuu41iGj"
      },
      "source": [
        "# Collect tweets\n",
        "tweets = tw.Cursor(api.search,\n",
        "              q=search_words,\n",
        "              lang=\"en\",\n",
        "              since=date_since).items(5)\n",
        "\n",
        "# Iterate and print tweets\n",
        "for tweet in tweets:\n",
        "    print(tweet.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3T1Vc831vTT"
      },
      "source": [
        "# Collect tweets\n",
        "tweets = tw.Cursor(api.search,\n",
        "                       q=search_words,\n",
        "                       lang=\"en\",\n",
        "                       since=date_since).items(5)\n",
        "\n",
        "# Collect a list of tweets\n",
        "[tweet.text for tweet in tweets]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
