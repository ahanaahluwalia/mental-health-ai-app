{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA with twitter trending.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/L8LwRudjU1o8yJX1xo7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joudelshawa/mental-health-ai-app/blob/feature%2Fdata_preprocessing/LDA_with_twitter_trending.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umNo6Hnh4hpB"
      },
      "source": [
        "import spacy\n",
        "spacy.load('en')\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('SCREEN_NAME')\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kGhYF36ED42",
        "outputId": "6d6c6564-f176-4d82-880b-eaa1e006a7be"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAWHZizsEGPE",
        "outputId": "851deae2-6c16-4988-fcf6-2b1e9cae50b6"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmk7mK9rEKaa"
      },
      "source": [
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "5XNllCfVEMCH",
        "outputId": "278d2ba5-344b-4309-ae6e-4e4828dacf95"
      },
      "source": [
        "import random\n",
        "text_data = []\n",
        "with open('dataset.csv') as f:\n",
        "    for line in f:\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        if random.random() > .99:\n",
        "            print(tokens)\n",
        "            text_data.append(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7369a1356984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_text_for_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2cDRWVmEN6g"
      },
      "source": [
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(text_data)corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "import pickle\n",
        "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "dictionary.save('dictionary.gensim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH5xaADhEQTV"
      },
      "source": [
        "import gensim\n",
        "NUM_TOPICS = 20\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "ldamodel.save('model5.gensim')\n",
        "topics = ldamodel.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7pH3CquEUpu"
      },
      "source": [
        "new_doc = 'Practical Bayesian Optimization of Machine Learning Algorithms'\n",
        "new_doc = prepare_text_for_lda(new_doc)\n",
        "new_doc_bow = dictionary.doc2bow(new_doc)\n",
        "print(new_doc_bow)\n",
        "print(ldamodel.get_document_topics(new_doc_bow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EXom8d3EXvS"
      },
      "source": [
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
        "ldamodel.save('model10.gensim')\n",
        "topics = ldamodel.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5490FW889P2-"
      },
      "source": [
        "pip install pycountry\n",
        "pip install nltk\n",
        "pip install textblob\n",
        "pip install wordcloud\n",
        "pip install scikit-learn\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "consumer_key = \"mdmzMBrRM2nNnTdEoN7jZuxmY\"\n",
        "consumer_key_secret = \"3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce\"\n",
        "access_token = \"525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP\"\n",
        "access_token_secret = \"eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQvxdzrtEGLV"
      },
      "source": [
        "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "# Define a function to get tweets\n",
        "def get_tweets(search, isHashtag):\n",
        "    \n",
        "    # Create a pandas DataFrame\n",
        "    df_temp = pd.DataFrame(columns=[\"Content\", \"Location\", \"Username\", \"Retweet-Count\", \"Favorites\", \"Created at\"])\n",
        "    \n",
        "    # Get the tweets\n",
        "    tweets = tweepy.Cursor(api.search, q= search+\" -filter:retweets\", lang=\"en\",since=\"2020-08-06\", tweet_mode='extended').items(10)\n",
        "    \n",
        "    # Iterate over tweets\n",
        "    for tweet in tweets:\n",
        "        content = tweet.full_text\n",
        "        username = tweet.user.screen_name\n",
        "        location = tweet.user.location\n",
        "        created_at = tweet.created_at\n",
        "        retweetcount = tweet.retweet_count\n",
        "        favorites = tweet.favorite_count\n",
        "        \n",
        "        # Create a list consists of the features\n",
        "        retrieved = [content, location, username, retweetcount, favorites, created_at]\n",
        "        \n",
        "        # Append list to the DataFrame\n",
        "        df_temp.loc[len(df_temp)] = retrieved\n",
        "        \n",
        "    # Generate unique filename\n",
        "    path = os.getcwd()\n",
        "    \n",
        "    # Generate a filename for hashtags or specific word\n",
        "    if isHashtag:\n",
        "        filename = path + '/drive/MyDrive/tweeter/' + search[1:] + '_hashtag.csv'\n",
        "    else:\n",
        "        filename = path + '/drive/MyDrive/tweeter/' + search.replace(\" \", \"\") + '_wordsearch.csv'\n",
        "    # Save the csv file\n",
        "    df_temp.to_csv(filename)\n",
        "\n",
        "\n",
        "# Call get_tweets function for each hashtag and search word\n",
        "\n",
        "for hashtag in hashtags:\n",
        "    get_tweets(hashtag, isHashtag=True)\n",
        "\n",
        "for search in search_list:\n",
        "    get_tweets(search, isHashtag=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
